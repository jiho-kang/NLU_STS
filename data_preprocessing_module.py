# -*- coding: utf-8 -*-
"""data_preprocessing_module.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PwQNa0F-oh0R1yy8pUalCnaouwGmmhRJ

-----data preprocessing-----
|   1. cleaning             |
|   2. back translation     |
|   3. data augmentation    |
|___________________________|

- settings:
    - module_path 본인 환경에 맞게 수정할 수 있도록 추후 업데이트 예정
    - !pip install konlpy : data augmentation module 사용을 위한 install
    - !pip install pororo : back translation module 사용을 위한 install ***지금은 사용 안함!!***

- class : data_preprosessing(path, flag, size, n)
    param : 
        - path              : origin data file path
        - flag              : STS/NLG
        - size=0.2          : train_valid split frac
        - n=2               : data augmentation times
        - random_stat=None  : train test split random state

- functions : 
        - cleaning_df()         : input=None, output=DataFrame
        - back_translation(df)  : input=(cleaned)DataFrame, output=DataFrame
        - data_aug(df)          : input=(cleaned)DataFrame, output=DataFrame
        - load_df()             : input=None, output=DataFrame train, valid test

- sample code :
    path = '/content/drive/MyDrive/wanted/data/' # data 저장된 path
    flag = 'STS'                                 # flag : STS/NLG
    size = 0.1                                   # 0.1 - 1
    pp = data_preprosessing(path, flag, size)    # class param : path, flag, size
    train, valid, test = pp.load_df()            # return result DataFrame
"""

import numpy as np
import pandas as pd

from gensim.summarization.summarizer import summarize
from konlpy.tag import Komoran

import tarfile
import sys
import json
from tqdm import tqdm
import re
import os

from sklearn.model_selection import train_test_split

def set_module_path():
    path = os.getcwd()
    print(path)
    return path

module_path = '/content/drive/MyDrive/wanted/modules'
sys.path.append(module_path)
import data_cleaning_module as dc
# import NLG_backtranslation as bt # 현재는 사용 안함(나중에 실제 translation까지 하게 된다면 실행)
import eda

class data_preprosessing():
    def __init__(self, path, flag, size=0.2, n=2, random_state=None):
        
        self.path = path                                                    # csv 파일 불러오고 저장할 path
        self.synonyms = pd.read_csv(path+'NIKLex_synonym.tsv', sep='\t')    # augmentation에서 사용할 말뭉치
        self.n = n                                                          # aumentation param
        self.size = size
        self.random_state = random_state
        if flag == 'STS':
            self.flag = flag
            self.file_n = 'https://raw.githubusercontent.com/KLUE-benchmark/KLUE/main/klue_benchmark/klue-sts-v1.1/klue-sts-v1.1_train.json'
            self.col1 = 'sentence1'
            self.col2 = 'sentence2'
            self.label = 'labels'
            self.bt_df = 'STS_backtranslation.csv'

        elif flag == 'NLG':
            self.flag = flag
            self.file_n = 'sports_news_data.csv'
            self.col1 = 'title'
            self.col2 = 'content'
            self.bt_df = 'NLG_backtranslation.csv'

    def data_check(self, df, col1=None, col2=None):
        # 결측치 제거
        df = df.dropna(axis=0)
        
        # 중복값 제거
        df = df.drop_duplicates([col1,col2], keep='first')

        # BT data에서 train 값 뽑기 위한 index reset
        df = df.reset_index(drop=True)

        return df

    def load_origin_file(self):
        if self.flag == 'STS':
            df = pd.read_json(self.file_n)

            # train label 추가
            df['binary'] = df['labels'].apply(lambda x: x['binary-label'])
            df['normalized'] = df['labels'].apply(lambda x: float(x['label']/5.0))
            df['score'] = df['labels'].apply(lambda x: x['label'])

            df = df.drop(columns='annotations', axis=1)
            
            # cleaning
            df = self.cleaning_df(df)
            
            # 결측치, 중복 제거
            df = self.data_check(df, self.col1, self.col2)
            
            # dev set
            test = pd.read_json('https://raw.githubusercontent.com/KLUE-benchmark/KLUE/main/klue_benchmark/klue-sts-v1.1/klue-sts-v1.1_dev.json')

            # dev label 추가
            test['binary'] = test['labels'].apply(lambda x: x['binary-label'])
            test['normalized'] = test['labels'].apply(lambda x: float(x['label']/5.0))
            test['score'] = test['labels'].apply(lambda x: x['label'])

        return df, test
    
    def read_origin_df(self):
        # original file load
        if self.flag == 'STS':
            df = pd.read_json(self.file_n)

            # train label 추가
            df['binary'] = df['labels'].apply(lambda x: x['binary-label'])
            df['normalized'] = df['labels'].apply(lambda x: float(x['label']/5.0))
            df['score'] = df['labels'].apply(lambda x: x['label'])

            df = df.drop(columns='annotations', axis=1)
            
            # cleaning
            df = self.cleaning_df(df)
            
            # 결측치, 중복 제거
            df = self.data_check(df, self.col1, self.col2)

            train, valid = train_test_split(df, test_size=self.size, random_state = self.random_state)
            
            # dev set
            test = pd.read_json('https://raw.githubusercontent.com/KLUE-benchmark/KLUE/main/klue_benchmark/klue-sts-v1.1/klue-sts-v1.1_dev.json')

            # dev label 추가
            test['binary'] = test['labels'].apply(lambda x: x['binary-label'])
            test['normalized'] = test['labels'].apply(lambda x: float(x['label']/5.0))
            test['score'] = test['labels'].apply(lambda x: x['label'])

            # cleaning
            test = self.cleaning_df(test)

            # 결측치, 중복 제거
            test = self.data_check(test, self.col1, self.col2)

        elif self.flag == 'NLG':
            df = pd.read_csv(self.file_n)
            df = df.rename(columns={'TITLE':self.col1, 'CONTENT':self.col2})
            df = df.loc[:, self.col1:self.col2]
            
            # cleaning
            df = self.cleaning_df(df)

            # label 생성
            df = self.make_labels(df)
            
            df = df.drop(columns='title', axis=1)

            self.col1 = 'content'
            self.col2 = 'summary'

            # 결측치, 중복 제거
            df = self.data_check(df, self.col1, self.col2)

            train , test = train_test_split(df, test_size=0.23, random_state = self.random_state)
            train, valid = train_test_split(train, test_size=self.size, random_state = self.random_state)

                                                        # STS       |   NLG
        print(f"Length of Clearnd DF : {len(df)}")      # 67,423     |   9,050
        print('Length of Train : ',len(train))          # 53,938     |   7,059
        print('Length of valid : ',len(valid))          # 53,938     |   -
        print('Length of Test : ',len(test))            # 519       |   1,991

        return train, valid, test

    def make_labels(self, df):
        df['summary']=None
        # df['len']=None
        # err_idx=[]
        for i in tqdm(range(len(df))):
            try:
                length=0
                for j in df['content'][i].split('.')[:3]: # 첫 세 문장의 token을 lenght에 저장
                    length+=len(j.split(' '))

                s = summarize(df['content'][i], word_count=length)  # length만큼의 word를 선정하여 추출
                
                if len(s) < 1:
                    df['summary'][i] = np.nan
                else:
                    s = re.sub('\n', ' ', s)
                    df['summary'][i] = s
                # df['len'][i]=length
            except:
                df['summary'][i] = np.nan
                # err_idx.append(i)  # error가 난 문장의 index를 추적
        return df


    def cleaning_df(self , df):
        # data cleaning

        _dc = dc.DataCleaning()
        if self.flag == 'STS':
            cl_df = _dc.make_cleaned_df(df, self.col1, self.col2, self.flag, self.label)
        elif self.flag == 'NLG':
            cl_df = _dc.make_cleaned_df(df, self.col1, self.col2, self.flag)
        
        return cl_df

    # def slicing_df(self, cl_df):
        
    #     cl_df1 = cl_df[:500]
    #     cl_df2 = cl_df[500:1000]
    #     cl_df3 = cl_df[1000:1500]
    #     cl_df4 = cl_df[1500:]
    #     cl_df_lst = [cl_df1, cl_df2, cl_df3, cl_df4]

    #     return cl_df_lst

    def back_translation(self, cl_df):
        
        # back translation을 모두 돌리는건 load의 문제가 있어 취합된 파일만 불러오기

        # cl_df_len = len(cl_df)
        # _bt = bt.Korean_backtranslation()

        # if cl_df_len > 1000:
        #     bt_df = pd.DataFrame(columns=[self.col1, self.col2])
        #     cl_df_lst = self.slicing_df(cl_df)
        #     for i, dataframe in enumerate(cl_df_lst):
        #         print('\n시작하지!', i+1, '번째 df!')
        #         dataframe.reset_index(inplace=True)
        #         _bt_df = _bt.execute(dataframe, [self.col1, self.col2]).drop(columns=['index'], axis=1)
        #         bt_df = pd.concat([bt_df, _bt_df])
        # else:
        #     bt_df = _bt.execute(cl_df, [self.col1, self.col2])
        
        # return bt_df

        bt_df = pd.read_csv(self.path + self.bt_df)

        # 취합된 BT data에서 train index row 추출
        indexes = cl_df.index

        for i, index in tqdm(enumerate(indexes)):

            error_idx = []
            if i == 0:
                t_df = bt_df.iloc[[index]]
            else:
                try:
                    t_df = t_df.append(bt_df.iloc[[index]])
                except:
                    error_idx.append(index)

        if self.flag == 'NLG':
            t_df = self.make_labels(t_df)

        return t_df

    def data_aug(self, cl_df):

        _eda = eda.NLPAugment(cl_df, self.synonyms)
        print(f'\ncolumn name : {self.col1}')
        da_df_1 = _eda.augment_df_to_rows(self.flag, self.col1, self.n)
        
        if self.flag == 'STS':
            print(f'\ncolumn name : {self.col2}')
            da_df_2 = _eda.augment_df_to_rows(self.flag, self.col2, self.n)
            da_df = pd.concat([da_df_1, da_df_2], ignore_index=True)

        elif self.flag == 'NLG':
            da_df = da_df_1
            da_df = self.make_labels(da_df)

        return da_df

    def load_train(self, train, valid, test, shuffle:bool=False):

        print('\n**************cleaning : train, valid, test**************\n')
        cl_train = self.cleaning_df(train)            # original train -> cleaning
        valid = self.cleaning_df(valid)                 # original test -> cleaning
        test = self.cleaning_df(test)                 # original test -> cleaning
        
        # data check null, duplicated
        cl_train = self.data_check(cl_train, self.col1, self.col2)
        valid = self.data_check(valid, self.col1, self.col2)
        test = self.data_check(test, self.col1, self.col2)
        print(f'\n**************cleaned train length : {len(cl_train)}, valid length : {len(valid)}, test length : {len(test)}**************\n')

        print('\n**************back translate : train**************\n')
        bt_df = self.back_translation(train)    # train -> back translation
        bt_df = self.cleaning_df(bt_df)         # back translation -> cleaning
        bt_df = self.data_check(bt_df, self.col1, self.col2)    # data check null, duplicated
        print(f'\n**************back translate train length : {len(bt_df)}**************\n')

        print('\n**************data augmentation : train**************\n')
        da_df = self.data_aug(cl_train)  # cleaned data -> data augmentation
        da_df = self.cleaning_df(da_df)         # data augmentation -> cleaning
        da_df = self.data_check(da_df, self.col1, self.col2)    # data check null, duplicated
        print(f'\n**************data augmentation train length : {len(da_df)}**************\n')

        print('\n**************concat : train**************\n')
        train = pd.concat([cl_train, bt_df, da_df], axis=0, ignore_index=True)
        train = self.data_check(train, self.col1, self.col2)    # data check null, duplicated
        print(f'\n**************concat train length : {len(train)}**************\n')

        print(f'\n**************shuffle train, valid, test**************\n')
        if shuffle == True:
            train = train.sample(frac=1).reset_index(drop=True)
            valid = valid.sample(frac=1).reset_index(drop=True)
            test = test.sample(frac=1).reset_index(drop=True)

        print('\n**************saving files : train, valid, test**************\n')
        train.to_csv(self.path + self.flag + '_' + 'train.csv', index=False)
        valid.to_csv(self.path + self.flag + '_' + 'valid.csv', index=False)
        test.to_csv(self.path  + self.flag + '_' + 'test.csv', index=False)

        print(f'\n**************train length : {len(train)}, valid length : {len(valid)}, test length : {len(test)}**************\n')

        return train, valid, test

    def load_splited_df(self, shuffle:bool=False):
        
        _train, valid, test = self.read_origin_df()

        print('\n**************cleaning : train, valid, test**************\n')
        cl_train = self.cleaning_df(_train)            # original train -> cleaning
        valid = self.cleaning_df(valid)                 # original test -> cleaning
        test = self.cleaning_df(test)                 # original test -> cleaning
        
        # data check null, duplicated
        cl_train = self.data_check(cl_train, self.col1, self.col2)
        valid = self.data_check(valid, self.col1, self.col2)
        test = self.data_check(test, self.col1, self.col2)
        print(f'\n**************cleaned train length : {len(cl_train)}, valid length : {len(valid)}, test length : {len(test)}**************\n')

        print('\n**************back translate : train**************\n')
        bt_df = self.back_translation(_train)    # train -> back translation
        bt_df = self.cleaning_df(bt_df)         # back translation -> cleaning
        bt_df = self.data_check(bt_df, self.col1, self.col2)    # data check null, duplicated
        print(f'\n**************back translate train length : {len(bt_df)}**************\n')

        print('\n**************data augmentation : train**************\n')
        da_df = self.data_aug(cl_train, nlg_f)  # cleaned data -> data augmentation
        da_df = self.cleaning_df(da_df)         # data augmentation -> cleaning
        da_df = self.data_check(da_df, self.col1, self.col2)    # data check null, duplicated
        print(f'\n**************data augmentation train length : {len(da_df)}**************\n')

        print('\n**************concat : train**************\n')
        train = pd.concat([cl_train, bt_df, da_df], axis=0, ignore_index=True)
        train = self.data_check(train, self.col1, self.col2)    # data check null, duplicated
        print(f'\n**************concat train length : {len(train)}**************\n')

        print(f'\n**************shuffle train, valid, test**************\n')
        if shuffle == True:
            train = train.sample(frac=1).reset_index(drop=True)
            valid = valid.sample(frac=1).reset_index(drop=True)
            test = test.sample(frac=1).reset_index(drop=True)

        print('\n**************saving files : train, valid, test**************\n')
        train.to_csv(self.path + self.flag + '_' + 'train.csv', index=False)
        valid.to_csv(self.path + self.flag + '_' + 'valid.csv', index=False)
        test.to_csv(self.path  + self.flag + '_' + 'test.csv', index=False)

        print(f'\n**************train length : {len(train)}, valid length : {len(valid)}, test length : {len(test)}**************\n')
        return train, valid, test